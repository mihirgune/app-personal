{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Seq2seq for Movie Dataset for Better Understanding.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"0jztb8Jgw6Yt"},"source":["## `Importing Libraries`"]},{"cell_type":"code","metadata":{"id":"jYZbEvCQgmxq"},"source":["# import all the necessary libraries\n","import codecs\n","import io\n","import os\n","import re\n","import numpy as np\n","import requests\n","from gensim.models import Word2Vec\n","from keras import Input, Model\n","from keras.activations import softmax\n","from keras.layers import Embedding, LSTM, Dense\n","from keras.optimizers import RMSprop\n","from keras.preprocessing.sequence import pad_sequences\n","from keras.utils import to_categorical\n","from keras_preprocessing.text import Tokenizer"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OPOTaPJnRdbc","executionInfo":{"status":"ok","timestamp":1601226201945,"user_tz":-330,"elapsed":4456,"user":{"displayName":"Sanket Sonje","photoUrl":"","userId":"05822068833245189120"}},"outputId":"a4df675a-8e5b-4a87-d376-a47ae137f0e8","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"0CkCV4ndxEju"},"source":["# `Data Preprocessing Data Handling`"]},{"cell_type":"code","metadata":{"id":"yutstrn_fBvb"},"source":["# Get the data from file\n","def get_all_conversations():\n","  all_conversations = []\n","  with codecs.open(\"/content/drive/My Drive/2020_Intern_03_VIIT_03_Chatbot/Coding Part (Implementation)/Seq2Seq Transformer/Cornell Movie Dataset/cornell movie-dialogs corpus/movie_lines.txt\", \"rb\", encoding=\"utf-8\", errors=\"ignore\") as f:\n","  \n","    # split corpus line line\n","    lines = f.read().split(\"\\n\")\n","    \n","    # get each conversation\n","    for line in lines:\n","    \n","      # each line has multiple columns divided by '+++$+++'\n","      all_conversations.append(line.split(\" +++$+++ \"))\n","  \n","  # return all conversation\n","  return all_conversations"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9I8dwiNwgQpL"},"source":["# Dataset is too big hence taking only first 10000 lines\n","# create a function to get all sorted conversation\n","def get_all_sorted_chats(all_conversations):\n","    all_chats = {}\n","    for tokens in all_conversations[:6500]:\n","\n","        # if the line is valid - it contains all the metadata\n","        if len(tokens) > 4:\n","\n","            # save the line number and the text itself\n","            # 4 th is the index where actual dialogue is present    \n","            all_chats[int(tokens[0][1:])] = tokens[4]\n","\n","    # then sort the result and return list of tuples\n","    return sorted(all_chats.items(), key=lambda x: x[0])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EMtibq7Yhtxj"},"source":["# create a function to clean the text\n","def clean_text(text_to_clean):\n","\n","  # apply all these conditions to clean the text\n","  res = text_to_clean.lower()\n","  res = re.sub(r\"i'm\", \"i am\", res)\n","  res = re.sub(r\"he's\", \"he is\", res)\n","  res = re.sub(r\"she's\", \"she is\", res)\n","  res = re.sub(r\"it's\", \"it is\", res)\n","  res = re.sub(r\"that's\", \"that is\", res)\n","  res = re.sub(r\"what's\", \"what is\", res)\n","  res = re.sub(r\"where's\", \"where is\", res)\n","  res = re.sub(r\"how's\", \"how is\", res)\n","  res = re.sub(r\"\\'ll\", \" will\", res)\n","  res = re.sub(r\"\\'ve\", \" have\", res)\n","  res = re.sub(r\"\\'re\", \" are\", res)\n","  res = re.sub(r\"\\'d\", \" would\", res)\n","  res = re.sub(r\"\\'re\", \" are\", res)\n","  res = re.sub(r\"won't\", \"will not\", res)\n","  res = re.sub(r\"can't\", \"cannot\", res)\n","  res = re.sub(r\"n't\", \" not\", res)\n","  res = re.sub(r\"n'\", \"ng\", res)\n","  res = re.sub(r\"'bout\", \"about\", res)\n","  res = re.sub(r\"'til\", \"until\", res)\n","  res = re.sub(r\"[-()\\\"#/@;:<>{}`+=~|.!?,]\", \"\", res)\n","  \n","  # return the clean text\n","  return res"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OxlLcD-riOIg"},"source":["# create a function to group the lines into conversations\n","def get_conversation_dict(sorted_chats):\n","  \n","  # create a conversation dictionary to store the index and dialouge\n","  conversation_dict = {}\n","  \n","  # create a temporary counter\n","  counter = 1\n","\n","  # store all index to one list\n","  conversation_ids = []\n","\n","  # iterate through all sorted conversations\n","  for i in range(1, len(sorted_chats) + 1):\n","\n","    # for all conversations index range between 1 to len(sorted_chats)\n","    if i < len(sorted_chats):\n","\n","      # if the current line number differs from the previous only by 1\n","      if (sorted_chats[i][0] - sorted_chats[i - 1][0]) == 1:\n","        \n","        # then this line is a part of the current conversation\n","        # if the previous line was not added before,\n","        # then we should add it now\n","        if sorted_chats[i - 1][1] not in conversation_ids:\n","          conversation_ids.append(sorted_chats[i - 1][1])\n","        \n","        # or just append the current line\n","        conversation_ids.append(sorted_chats[i][1])\n","            \n","      # If the difference is more than 1\n","      # it means new conversation has started and we should clear conversation_ids\n","      elif (sorted_chats[i][0] - sorted_chats[i - 1][0]) > 1:\n","        conversation_dict[counter] = conversation_ids\n","        conversation_ids = []\n","        counter += 1\n","      else:\n","        continue\n","\n","  # return conversation dictionary with all conversations   \n","  return conversation_dict"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YFo0PPHnsgsL"},"source":["# create a function to prepare the list of questions and answers\n","def get_clean_q_and_a(conversations_dictionary):\n","\n","  # Create an questions and answers list\n","  questions_and_answer = []\n","  \n","  # iterate through each conversation\n","  for current_conversation in conversations_dictionary.values():\n","    \n","    # make sure that each conversation contains an even number of lines\n","    if len(current_conversation) % 2 != 0:\n","      current_conversation = current_conversation[:-1]\n","\n","    # convert questions and answers to the list of tuples\n","    for i in range(0, len(current_conversation), 2):\n","      questions_and_answer.append((current_conversation[i], current_conversation[i + 1]))\n","\n","  # zip with * operator unzips tuples into independent lists\n","  questions, answers = zip(*questions_and_answer)\n","  \n","  # get the list of the questions\n","  questions_list = list(questions)\n","\n","  # clear questions from contracted forms, non-letter symbols and convert it to lowercase\n","  clean_questions = list()\n","    \n","  for i in range(len(questions_list)):\n","    clean_questions.append(clean_text(questions_list[i]))\n","\n","  # get the list of the answers\n","  answer_list = list(answers)\n","\n","  # do the same with the answers, but now we need to add 'start' and 'end' words\n","  clean_answers = list()\n","  \n","  for i in range(len(answer_list)):\n","    clean_answers.append('<START> ' + clean_text(answer_list[i]) + ' <END>')\n","  \n","  # return clean answers and clean questions\n","  return clean_questions, clean_answers"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Y8qoneumuYXa","executionInfo":{"status":"ok","timestamp":1601226208157,"user_tz":-330,"elapsed":2516,"user":{"displayName":"Sanket Sonje","photoUrl":"","userId":"05822068833245189120"}},"outputId":"bbb6730a-ddc5-4558-d633-2c5bff62bae6","colab":{"base_uri":"https://localhost:8080/","height":68}},"source":["# run all the function to check for progress\n","\n","# get all the conversation from dataset\n","conversations = get_all_conversations()\n","\n","# get the total conversation length\n","total = len(conversations)\n","print(\"Total conversations in dataset: {}\".format(total))\n","\n","# get all the sorted conversation\n","all_sorted_chats = get_all_sorted_chats(conversations)\n","\n","# get the conversation dictionary\n","conversation_dictionary = get_conversation_dict(all_sorted_chats)\n","\n","# get the list of questions and answers\n","questions, answers = get_clean_q_and_a(conversation_dictionary)\n","\n","# print total number of questions and answers\n","print(\"Questions in dataset: {}\".format(len(questions)))\n","print(\"Answers in dataset: {}\".format(len(answers)))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Total conversations in dataset: 304714\n","Questions in dataset: 3041\n","Answers in dataset: 3041\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"oYOsWIEYxaog"},"source":["## `Base Model Training`"]},{"cell_type":"code","metadata":{"id":"87Hlv8iyudIH","executionInfo":{"status":"ok","timestamp":1601226210884,"user_tz":-330,"elapsed":916,"user":{"displayName":"Sanket Sonje","photoUrl":"","userId":"05822068833245189120"}},"outputId":"a6004597-e99d-4952-c008-4a26ae885ec8","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# main regular expression\n","target_regex = '!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~\\t\\n\\'0123456789'\n","\n","# Tokenizer allows to vectorize our corpus by turning each sentence into a sequence of integers where each integer is an index\n","# of a token in an internal dictionary\n","tokenizer = Tokenizer(filters=target_regex)\n","tokenizer.fit_on_texts(questions + answers)\n","\n","# get the vocab size\n","VOCAB_SIZE = len(tokenizer.word_index) + 1\n","print('Vocabulary size : {}'.format(VOCAB_SIZE))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Vocabulary size : 6077\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"cgr4Wo569nll","executionInfo":{"status":"ok","timestamp":1601226212311,"user_tz":-330,"elapsed":895,"user":{"displayName":"Sanket Sonje","photoUrl":"","userId":"05822068833245189120"}},"outputId":"c24fecb3-923d-499e-d974-656205faf15c","colab":{"base_uri":"https://localhost:8080/","height":119}},"source":["# Prepare two matrices from the lists of questions and answers\n","# use OHE for padding\n","\n","# tokenized and add padding to questions\n","tokenized_questions = tokenizer.texts_to_sequences(questions)\n","maxlen_questions = max([len(x) for x in tokenized_questions])\n","print(\"\\nMax length of questions is :- \",maxlen_questions)\n","\n","# pad each question with zeros at the end to be 223 words long\n","encoder_input_data = pad_sequences(tokenized_questions, maxlen=maxlen_questions, padding='post')\n","\n","# matrix of 4709x223 integers - 4709 questions 223 words each\n","print(encoder_input_data.shape)\n","\n","# tokenized and add padding to questions\n","tokenized_answers = tokenizer.texts_to_sequences(answers)\n","maxlen_answers = max([len(x) for x in tokenized_answers])\n","print(\"\\nMax length of answers is :- \",maxlen_answers)\n","\n","# pad each answer with zeros at the end to be 132 words long\n","decoder_input_data = pad_sequences(tokenized_answers, maxlen=maxlen_answers, padding='post')\n","\n","# matrix of 4709x132 integers - 4709 answers 132 words each\n","print(decoder_input_data.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\n","Max length of questions is :-  223\n","(3041, 223)\n","\n","Max length of questions is :-  132\n","(3041, 132)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"0i4gKGj_96j4"},"source":["# Create one-hot encoded answers\n","\n","# remove the first 'start' word from every answer\n","for i in range(len(tokenized_answers)):\n","  tokenized_answers[i] = tokenized_answers[i][1:]\n","\n","# pad answers with zeros\n","padded_answers = pad_sequences(tokenized_answers, maxlen=maxlen_answers, padding='post')\n","\n","# tensor of size (4709, 132, 7910)\n","# 4709 answers 132 words each, and each word is one-hot encoded using our vocabulary\n","decoder_output_data = to_categorical(padded_answers, VOCAB_SIZE)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Tw9PmXiyAf_2","executionInfo":{"status":"ok","timestamp":1601226228522,"user_tz":-330,"elapsed":2964,"user":{"displayName":"Sanket Sonje","photoUrl":"","userId":"05822068833245189120"}},"outputId":"5e9777e2-8d5c-4029-e516-6c649ef2d9ad","colab":{"base_uri":"https://localhost:8080/","height":425}},"source":["# encoder will be used to capture space-dependent \n","# relations between words from the questions\n","enc_inputs = Input(shape=(None,))\n","enc_embedding = Embedding(VOCAB_SIZE, 200, mask_zero=True)(enc_inputs)\n","enc_outputs, state_h, state_c = LSTM(200, return_state=True)(enc_embedding)\n","enc_states = [state_h, state_c]\n","\n","# decoder will be used to capture space-dependent relations between words from the answers using encoder's internal state as a context\n","dec_inputs = Input(shape=(None,))\n","dec_embedding = Embedding(VOCAB_SIZE, 200, mask_zero=True)(dec_inputs)\n","dec_lstm = LSTM(200, return_state=True, return_sequences=True)\n","dec_outputs, _, _ = dec_lstm(dec_embedding, initial_state=enc_states)\n","\n","# decoder is connected to the output Dense layer\n","dec_dense = Dense(VOCAB_SIZE, activation=softmax)\n","output = dec_dense(dec_outputs)\n","model = Model([enc_inputs, dec_inputs], output)\n","\n","# output of this network will look like this:\n","# y_true = [0.05, 0.95, 0...]\n","# and expected one-hot encoded output like this:\n","# y_pred = [0, 1, 0...]\n","model.compile(optimizer=RMSprop(), loss='categorical_crossentropy')\n","model.summary()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Model: \"functional_1\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_1 (InputLayer)            [(None, None)]       0                                            \n","__________________________________________________________________________________________________\n","input_2 (InputLayer)            [(None, None)]       0                                            \n","__________________________________________________________________________________________________\n","embedding (Embedding)           (None, None, 200)    1215400     input_1[0][0]                    \n","__________________________________________________________________________________________________\n","embedding_1 (Embedding)         (None, None, 200)    1215400     input_2[0][0]                    \n","__________________________________________________________________________________________________\n","lstm (LSTM)                     [(None, 200), (None, 320800      embedding[0][0]                  \n","__________________________________________________________________________________________________\n","lstm_1 (LSTM)                   [(None, None, 200),  320800      embedding_1[0][0]                \n","                                                                 lstm[0][1]                       \n","                                                                 lstm[0][2]                       \n","__________________________________________________________________________________________________\n","dense (Dense)                   (None, None, 6077)   1221477     lstm_1[0][0]                     \n","==================================================================================================\n","Total params: 4,293,877\n","Trainable params: 4,293,877\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"dEdQuXjpC369","executionInfo":{"status":"ok","timestamp":1601033941248,"user_tz":-330,"elapsed":679265,"user":{"displayName":"Sanket Sonje","photoUrl":"","userId":"05822068833245189120"}},"outputId":"77d10c95-e98e-4fe5-f0f4-34c929ac6e7a","colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["model.fit([encoder_input_data, decoder_input_data], decoder_output_data, batch_size=50, epochs=100)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 1/100\n","10/10 [==============================] - 6s 607ms/step - loss: 1.1843\n","Epoch 2/100\n","10/10 [==============================] - 6s 614ms/step - loss: 1.0525\n","Epoch 3/100\n","10/10 [==============================] - 6s 594ms/step - loss: 1.0273\n","Epoch 4/100\n","10/10 [==============================] - 6s 592ms/step - loss: 1.0055\n","Epoch 5/100\n","10/10 [==============================] - 6s 591ms/step - loss: 0.9877\n","Epoch 6/100\n","10/10 [==============================] - 6s 593ms/step - loss: 0.9692\n","Epoch 7/100\n","10/10 [==============================] - 6s 594ms/step - loss: 0.9565\n","Epoch 8/100\n","10/10 [==============================] - 6s 593ms/step - loss: 0.9453\n","Epoch 9/100\n","10/10 [==============================] - 6s 590ms/step - loss: 0.9332\n","Epoch 10/100\n","10/10 [==============================] - 6s 586ms/step - loss: 0.9235\n","Epoch 11/100\n","10/10 [==============================] - 6s 591ms/step - loss: 0.9140\n","Epoch 12/100\n","10/10 [==============================] - 6s 600ms/step - loss: 0.9051\n","Epoch 13/100\n","10/10 [==============================] - 7s 666ms/step - loss: 0.8961\n","Epoch 14/100\n","10/10 [==============================] - 6s 639ms/step - loss: 0.8875\n","Epoch 15/100\n","10/10 [==============================] - 8s 783ms/step - loss: 0.8786\n","Epoch 16/100\n","10/10 [==============================] - 7s 701ms/step - loss: 0.8704\n","Epoch 17/100\n","10/10 [==============================] - 6s 608ms/step - loss: 0.8608\n","Epoch 18/100\n","10/10 [==============================] - 6s 588ms/step - loss: 0.8513\n","Epoch 19/100\n","10/10 [==============================] - 6s 600ms/step - loss: 0.8429\n","Epoch 20/100\n","10/10 [==============================] - 6s 594ms/step - loss: 0.8329\n","Epoch 21/100\n","10/10 [==============================] - 6s 599ms/step - loss: 0.8237\n","Epoch 22/100\n","10/10 [==============================] - 6s 597ms/step - loss: 0.8143\n","Epoch 23/100\n","10/10 [==============================] - 6s 592ms/step - loss: 0.8057\n","Epoch 24/100\n","10/10 [==============================] - 6s 598ms/step - loss: 0.7959\n","Epoch 25/100\n","10/10 [==============================] - 6s 596ms/step - loss: 0.7893\n","Epoch 26/100\n","10/10 [==============================] - 6s 597ms/step - loss: 0.7781\n","Epoch 27/100\n","10/10 [==============================] - 6s 593ms/step - loss: 0.7687\n","Epoch 28/100\n","10/10 [==============================] - 6s 592ms/step - loss: 0.7608\n","Epoch 29/100\n","10/10 [==============================] - 6s 590ms/step - loss: 0.7520\n","Epoch 30/100\n","10/10 [==============================] - 6s 591ms/step - loss: 0.7423\n","Epoch 31/100\n","10/10 [==============================] - 6s 592ms/step - loss: 0.7323\n","Epoch 32/100\n","10/10 [==============================] - 6s 587ms/step - loss: 0.7252\n","Epoch 33/100\n","10/10 [==============================] - 6s 597ms/step - loss: 0.7158\n","Epoch 34/100\n","10/10 [==============================] - 6s 591ms/step - loss: 0.7050\n","Epoch 35/100\n","10/10 [==============================] - 6s 592ms/step - loss: 0.6970\n","Epoch 36/100\n","10/10 [==============================] - 6s 589ms/step - loss: 0.6878\n","Epoch 37/100\n","10/10 [==============================] - 6s 590ms/step - loss: 0.6793\n","Epoch 38/100\n","10/10 [==============================] - 6s 592ms/step - loss: 0.6689\n","Epoch 39/100\n","10/10 [==============================] - 6s 596ms/step - loss: 0.6640\n","Epoch 40/100\n","10/10 [==============================] - 6s 588ms/step - loss: 0.6507\n","Epoch 41/100\n","10/10 [==============================] - 6s 587ms/step - loss: 0.6430\n","Epoch 42/100\n","10/10 [==============================] - 6s 593ms/step - loss: 0.6327\n","Epoch 43/100\n","10/10 [==============================] - 6s 592ms/step - loss: 0.6243\n","Epoch 44/100\n","10/10 [==============================] - 6s 594ms/step - loss: 0.6156\n","Epoch 45/100\n","10/10 [==============================] - 6s 587ms/step - loss: 0.6053\n","Epoch 46/100\n","10/10 [==============================] - 6s 591ms/step - loss: 0.5996\n","Epoch 47/100\n","10/10 [==============================] - 6s 594ms/step - loss: 0.5871\n","Epoch 48/100\n","10/10 [==============================] - 6s 588ms/step - loss: 0.5793\n","Epoch 49/100\n","10/10 [==============================] - 6s 590ms/step - loss: 0.5709\n","Epoch 50/100\n","10/10 [==============================] - 6s 586ms/step - loss: 0.5614\n","Epoch 51/100\n","10/10 [==============================] - 6s 592ms/step - loss: 0.5535\n","Epoch 52/100\n","10/10 [==============================] - 6s 589ms/step - loss: 0.5435\n","Epoch 53/100\n","10/10 [==============================] - 6s 590ms/step - loss: 0.5350\n","Epoch 54/100\n","10/10 [==============================] - 6s 583ms/step - loss: 0.5255\n","Epoch 55/100\n","10/10 [==============================] - 6s 588ms/step - loss: 0.5171\n","Epoch 56/100\n","10/10 [==============================] - 6s 597ms/step - loss: 0.5074\n","Epoch 57/100\n","10/10 [==============================] - 6s 599ms/step - loss: 0.4986\n","Epoch 58/100\n","10/10 [==============================] - 6s 590ms/step - loss: 0.4925\n","Epoch 59/100\n","10/10 [==============================] - 6s 592ms/step - loss: 0.4802\n","Epoch 60/100\n","10/10 [==============================] - 6s 606ms/step - loss: 0.4727\n","Epoch 61/100\n","10/10 [==============================] - 7s 686ms/step - loss: 0.4635\n","Epoch 62/100\n","10/10 [==============================] - 11s 1s/step - loss: 0.4550\n","Epoch 63/100\n","10/10 [==============================] - 7s 651ms/step - loss: 0.4463\n","Epoch 64/100\n","10/10 [==============================] - 6s 594ms/step - loss: 0.4380\n","Epoch 65/100\n","10/10 [==============================] - 6s 604ms/step - loss: 0.4285\n","Epoch 66/100\n","10/10 [==============================] - 6s 601ms/step - loss: 0.4206\n","Epoch 67/100\n","10/10 [==============================] - 6s 591ms/step - loss: 0.4113\n","Epoch 68/100\n","10/10 [==============================] - 6s 605ms/step - loss: 0.4013\n","Epoch 69/100\n","10/10 [==============================] - 6s 598ms/step - loss: 0.3972\n","Epoch 70/100\n","10/10 [==============================] - 6s 601ms/step - loss: 0.3853\n","Epoch 71/100\n","10/10 [==============================] - 6s 594ms/step - loss: 0.3748\n","Epoch 72/100\n","10/10 [==============================] - 6s 595ms/step - loss: 0.3705\n","Epoch 73/100\n","10/10 [==============================] - 6s 594ms/step - loss: 0.3605\n","Epoch 74/100\n","10/10 [==============================] - 6s 610ms/step - loss: 0.3517\n","Epoch 75/100\n","10/10 [==============================] - 6s 593ms/step - loss: 0.3467\n","Epoch 76/100\n","10/10 [==============================] - 6s 594ms/step - loss: 0.3342\n","Epoch 77/100\n","10/10 [==============================] - 6s 608ms/step - loss: 0.3266\n","Epoch 78/100\n","10/10 [==============================] - 6s 599ms/step - loss: 0.3193\n","Epoch 79/100\n","10/10 [==============================] - 6s 609ms/step - loss: 0.3136\n","Epoch 80/100\n","10/10 [==============================] - 6s 596ms/step - loss: 0.3058\n","Epoch 81/100\n","10/10 [==============================] - 6s 601ms/step - loss: 0.2958\n","Epoch 82/100\n","10/10 [==============================] - 6s 593ms/step - loss: 0.2874\n","Epoch 83/100\n","10/10 [==============================] - 6s 611ms/step - loss: 0.2817\n","Epoch 84/100\n","10/10 [==============================] - 6s 598ms/step - loss: 0.2718\n","Epoch 85/100\n","10/10 [==============================] - 6s 593ms/step - loss: 0.2671\n","Epoch 86/100\n","10/10 [==============================] - 6s 605ms/step - loss: 0.2588\n","Epoch 87/100\n","10/10 [==============================] - 6s 594ms/step - loss: 0.2514\n","Epoch 88/100\n","10/10 [==============================] - 6s 592ms/step - loss: 0.2452\n","Epoch 89/100\n","10/10 [==============================] - 6s 593ms/step - loss: 0.2377\n","Epoch 90/100\n","10/10 [==============================] - 6s 593ms/step - loss: 0.2300\n","Epoch 91/100\n","10/10 [==============================] - 6s 585ms/step - loss: 0.2231\n","Epoch 92/100\n","10/10 [==============================] - 6s 598ms/step - loss: 0.2181\n","Epoch 93/100\n","10/10 [==============================] - 6s 588ms/step - loss: 0.2089\n","Epoch 94/100\n","10/10 [==============================] - 6s 586ms/step - loss: 0.2050\n","Epoch 95/100\n","10/10 [==============================] - 6s 595ms/step - loss: 0.1964\n","Epoch 96/100\n","10/10 [==============================] - 6s 592ms/step - loss: 0.1917\n","Epoch 97/100\n","10/10 [==============================] - 6s 598ms/step - loss: 0.1853\n","Epoch 98/100\n","10/10 [==============================] - 6s 589ms/step - loss: 0.1782\n","Epoch 99/100\n","10/10 [==============================] - 6s 589ms/step - loss: 0.1718\n","Epoch 100/100\n","10/10 [==============================] - 6s 592ms/step - loss: 0.1666\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<tensorflow.python.keras.callbacks.History at 0x7fa417c36da0>"]},"metadata":{"tags":[]},"execution_count":14}]},{"cell_type":"markdown","metadata":{"id":"542KNkAR9okq"},"source":["## `Inference Model`"]},{"cell_type":"code","metadata":{"id":"VVeFPrvN0Rxn"},"source":["# create an inference model encoder\n","def make_inference_models():\n","  # two inputs for the state vectors returned by encoder\n","  dec_state_input_h = Input(shape=(200,))\n","  dec_state_input_c = Input(shape=(200,))\n","  dec_states_inputs = [dec_state_input_h, dec_state_input_c]\n","    \n","  # these state vectors are used as an initial state  \n","  # for LSTM layer in the inference decoder\n","  # third input is the Embedding layer as explained above   \n","  dec_outputs, state_h, state_c = dec_lstm(dec_embedding, initial_state=dec_states_inputs)\n","  dec_states = [state_h, state_c]\n","    \n","  # Dense layer is used to return OHE predicted word\n","  dec_outputs = dec_dense(dec_outputs)\n","  dec_model = Model(inputs=[dec_inputs] + dec_states_inputs, outputs=[dec_outputs] + dec_states)\n","   \n","  # single encoder input is a question, represented as a sequence \n","  # of integers padded with zeros\n","  enc_model = Model(inputs=enc_inputs, outputs=enc_states)\n","   \n","  return enc_model, dec_model\n","\n","# run the above function to get the encoding and decoding sequence\n","enc_model, dec_model = make_inference_models()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"u0jHtw6O1WNG"},"source":["# Create a function to convert the string into tokens\n","def str_to_tokens(sentence: str):\n","  # convert input string to lowercase, then split it by whitespaces\n","  words = sentence.lower().split()\n","    \n","  # then convert to a sequence of integers padded with zeros\n","  tokens_list = list()\n","  for current_word in words:\n","    result = tokenizer.word_index.get(current_word, '')\n","\n","    # if list is not empty then append the result into token_list\n","    if result != '':\n","      tokens_list.append(result)\n","\n","  # return One Hot Encodding of input string\n","  return pad_sequences([tokens_list], maxlen=maxlen_questions, padding='post')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fWCK3v-E9ugj"},"source":["# `Chatbot Loop`"]},{"cell_type":"code","metadata":{"id":"2FXiWBh41XC4"},"source":["def Predictions(inputText):\n","  # main chatbot questions and answers\n","  # encode the input sequence into state vectors\n","  input_query = inputText\n","  # input_query = input('\\nEnter question : ')\n","\n","  # to continue the conversation\n","  states_values = enc_model.predict(str_to_tokens(input_query))\n","\n","  # start with a target sequence of size 1 - word 'start'   \n","  empty_target_seq = np.zeros((1, 1))\n","  empty_target_seq[0, 0] = tokenizer.word_index['start']\n","  stop_condition = False\n","  decoded_translation = ''\n","\n","  # loop until true to apply text generation algorithm\n","  while not stop_condition:\n","            \n","    # feed the state vectors and 1-word target sequence to the decoder to produce predictions for the next word\n","    dec_outputs, h, c = dec_model.predict([empty_target_seq] + states_values)         \n","            \n","    # sample the next word using these predictions\n","    sampled_word_index = np.argmax(dec_outputs[0, -1, :])\n","    sampled_word = None\n","            \n","    # append the sampled word to the target sequence\n","    for word, index in tokenizer.word_index.items():\n","      if sampled_word_index == index:\n","        if word != 'end':\n","          decoded_translation += ' {}'.format(word)\n","        sampled_word = word\n","            \n","    # repeat until we generate the end-of-sequence word 'end' or we hit the length of answer limit\n","    if sampled_word == 'end' or len(decoded_translation.split()) > maxlen_answers:\n","      stop_condition = True\n","            \n","    # prepare next iteration\n","    empty_target_seq = np.zeros((1, 1))\n","    empty_target_seq[0, 0] = sampled_word_index\n","    states_values = [h, c]\n","        \n","  # print(\"Chatbot        :\",decoded_translation)\n","  return decoded_translation"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3ooK56aKE9jO","executionInfo":{"status":"ok","timestamp":1601033944664,"user_tz":-330,"elapsed":675166,"user":{"displayName":"Sanket Sonje","photoUrl":"","userId":"05822068833245189120"}},"outputId":"05a50b35-460a-4a9f-a701-60252ab34755","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# testting for pickle\n","input = \"hi\"\n","print(Predictions(input))"],"execution_count":null,"outputs":[{"output_type":"stream","text":[" you are not a big talker are you\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"0_4e9Vgc8tpp"},"source":["# Use pickle to load in the pre-trained model\n","import pickle\n","from threading import Thread\n","\n","with open('model_1.pkl', 'wb') as file:\n","  pickle.dump('model_1.pkl', file)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AmXAAE_n9M8t","executionInfo":{"status":"ok","timestamp":1601033949400,"user_tz":-330,"elapsed":677919,"user":{"displayName":"Sanket Sonje","photoUrl":"","userId":"05822068833245189120"}},"outputId":"71aed387-fa81-4e12-ccbb-118a6252ea74","colab":{"base_uri":"https://localhost:8080/","height":272}},"source":["!pip install flask-ngrok"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting flask-ngrok\n","  Downloading https://files.pythonhosted.org/packages/af/6c/f54cb686ad1129e27d125d182f90f52b32f284e6c8df58c1bae54fa1adbc/flask_ngrok-0.0.25-py3-none-any.whl\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from flask-ngrok) (2.23.0)\n","Requirement already satisfied: Flask>=0.8 in /usr/local/lib/python3.6/dist-packages (from flask-ngrok) (1.1.2)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->flask-ngrok) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->flask-ngrok) (2020.6.20)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->flask-ngrok) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->flask-ngrok) (3.0.4)\n","Requirement already satisfied: Jinja2>=2.10.1 in /usr/local/lib/python3.6/dist-packages (from Flask>=0.8->flask-ngrok) (2.11.2)\n","Requirement already satisfied: Werkzeug>=0.15 in /usr/local/lib/python3.6/dist-packages (from Flask>=0.8->flask-ngrok) (1.0.1)\n","Requirement already satisfied: click>=5.1 in /usr/local/lib/python3.6/dist-packages (from Flask>=0.8->flask-ngrok) (7.1.2)\n","Requirement already satisfied: itsdangerous>=0.24 in /usr/local/lib/python3.6/dist-packages (from Flask>=0.8->flask-ngrok) (1.1.0)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from Jinja2>=2.10.1->Flask>=0.8->flask-ngrok) (1.1.1)\n","Installing collected packages: flask-ngrok\n","Successfully installed flask-ngrok-0.0.25\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"V57ozEEiVqbn"},"source":["import numpy as np\n","from flask import Flask, request, jsonify, render_template\n","import pickle\n","import threading\n","from flask_ngrok import run_with_ngrok"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"g3Iv50GtVr1h","executionInfo":{"status":"ok","timestamp":1601033949405,"user_tz":-330,"elapsed":676332,"user":{"displayName":"Sanket Sonje","photoUrl":"","userId":"05822068833245189120"}},"outputId":"0113fb84-03cd-4e6a-8f15-ed216b5cdc82","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["import socket\n","print(socket.gethostbyname(socket.gethostname()))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["172.28.0.2\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"y023R52fVtFs"},"source":["app = Flask(__name__,template_folder='/content/drive/My Drive/2020_Intern_03_VIIT_03_Chatbot/Coding Part (Implementation)/Seq2Seq Transformer/Flask/templates')\n","run_with_ngrok(app)\n","model = pickle.load(open('/content/drive/My Drive/2020_Intern_03_VIIT_03_Chatbot/Coding Part (Implementation)/Seq2Seq Transformer/Flask/model_1.pkl', 'rb'))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nJ4bxZR-Vuqj"},"source":["@app.route('/')\n","def home():\n","  return render_template('index.html')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"j0q5UinCVv7D"},"source":["@app.route('/predict',methods=['POST'])\n","def predict():\n","  '''\n","  For rendering results on HTML GUI\n","  '''\n","    \n","  user_input = request.form.get(\"user_query\")\n","  prediction = Predictions(user_input)\n","\n","  output = prediction\n","\n","  return render_template('index.html', prediction_text=' {}'.format(output))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jCBr90N8Vyqe","executionInfo":{"status":"ok","timestamp":1601035382254,"user_tz":-330,"elapsed":60264,"user":{"displayName":"Sanket Sonje","photoUrl":"","userId":"05822068833245189120"}},"outputId":"4f912f5d-24c8-4215-bf02-e5d5d75e5674","colab":{"base_uri":"https://localhost:8080/","height":272}},"source":["if __name__ == \"__main__\":\n","  app.run()"],"execution_count":null,"outputs":[{"output_type":"stream","text":[" * Serving Flask app \"__main__\" (lazy loading)\n"," * Environment: production\n","\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n","\u001b[2m   Use a production WSGI server instead.\u001b[0m\n"," * Debug mode: off\n"],"name":"stdout"},{"output_type":"stream","text":[" * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n"],"name":"stderr"},{"output_type":"stream","text":[" * Running on http://5cc6ed40e811.ngrok.io\n"," * Traffic stats available on http://127.0.0.1:4040\n"],"name":"stdout"},{"output_type":"stream","text":["127.0.0.1 - - [25/Sep/2020 12:02:15] \"\u001b[37mGET / HTTP/1.1\u001b[0m\" 200 -\n","127.0.0.1 - - [25/Sep/2020 12:02:16] \"\u001b[33mGET /static/css/style.css HTTP/1.1\u001b[0m\" 404 -\n","127.0.0.1 - - [25/Sep/2020 12:02:17] \"\u001b[33mGET /favicon.ico HTTP/1.1\u001b[0m\" 404 -\n","127.0.0.1 - - [25/Sep/2020 12:02:29] \"\u001b[37mPOST /predict HTTP/1.1\u001b[0m\" 200 -\n","127.0.0.1 - - [25/Sep/2020 12:02:30] \"\u001b[33mGET /static/css/style.css HTTP/1.1\u001b[0m\" 404 -\n","127.0.0.1 - - [25/Sep/2020 12:02:52] \"\u001b[37mPOST /predict HTTP/1.1\u001b[0m\" 200 -\n","127.0.0.1 - - [25/Sep/2020 12:02:52] \"\u001b[33mGET /static/css/style.css HTTP/1.1\u001b[0m\" 404 -\n"],"name":"stderr"}]}]}