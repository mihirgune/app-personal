{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Flask for Andriod.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNitnMun36mtKXg0c/2yv52"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"38EsqSO5ek-b","executionInfo":{"status":"ok","timestamp":1601149995408,"user_tz":-330,"elapsed":1556,"user":{"displayName":"Sanket Sonje","photoUrl":"","userId":"05822068833245189120"}}},"source":["# Flask File for Andriod"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"Anmogg0ahpl8","executionInfo":{"status":"ok","timestamp":1601149995411,"user_tz":-330,"elapsed":1538,"user":{"displayName":"Sanket Sonje","photoUrl":"","userId":"05822068833245189120"}},"outputId":"317cc25d-5895-455c-ab60-261bdc271a7e","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Nqj6rRzMdEhw","executionInfo":{"status":"ok","timestamp":1601149999058,"user_tz":-330,"elapsed":4901,"user":{"displayName":"Sanket Sonje","photoUrl":"","userId":"05822068833245189120"}}},"source":["# import all the necessary libraries\n","import codecs\n","import io\n","import os\n","import re\n","import numpy as np\n","import requests\n","from gensim.models import Word2Vec\n","from keras import Input, Model\n","from keras.activations import softmax\n","from keras.layers import Embedding, LSTM, Dense\n","from keras.optimizers import RMSprop\n","from keras.preprocessing.sequence import pad_sequences\n","from keras.utils import to_categorical\n","from keras_preprocessing.text import Tokenizer\n","import numpy as np\n","from flask import Flask, request, jsonify, render_template\n","import pickle\n","import threading\n","from tensorflow import keras\n","\n","# Get the data from file\n","def get_all_conversations():\n","    all_conversations = []\n","    with codecs.open(\"/content/drive/My Drive/2020_Intern_03_VIIT_03_Chatbot/Coding Part (Implementation)/Seq2Seq Transformer/Cornell Movie Dataset/cornell movie-dialogs corpus/movie_lines.txt\", \"rb\", encoding=\"utf-8\", errors=\"ignore\") as f:\n","    \n","        # split corpus line line\n","        lines = f.read().split(\"\\n\")\n","        \n","        # get each conversation\n","        for line in lines:\n","        \n","            # each line has multiple columns divided by '+++$+++'\n","            all_conversations.append(line.split(\" +++$+++ \"))\n","    \n","    # return all conversation\n","    return all_conversations\n","\n","# Dataset is too big hence taking only first 10000 lines\n","# create a function to get all sorted conversation\n","def get_all_sorted_chats(all_conversations):\n","    all_chats = {}\n","    for tokens in all_conversations[:2000]:\n","\n","        # if the line is valid - it contains all the metadata\n","        if len(tokens) > 4:\n","\n","            # save the line number and the text itself\n","            # 4 th is the index where actual dialogue is present    \n","            all_chats[int(tokens[0][1:])] = tokens[4]\n","\n","    # then sort the result and return list of tuples\n","    return sorted(all_chats.items(), key=lambda x: x[0])\n","\n","# create a function to clean the text\n","def clean_text(text_to_clean):\n","\n","    # apply all these conditions to clean the text\n","    res = text_to_clean.lower()\n","    res = re.sub(r\"i'm\", \"i am\", res)\n","    res = re.sub(r\"he's\", \"he is\", res)\n","    res = re.sub(r\"she's\", \"she is\", res)\n","    res = re.sub(r\"it's\", \"it is\", res)\n","    res = re.sub(r\"that's\", \"that is\", res)\n","    res = re.sub(r\"what's\", \"what is\", res)\n","    res = re.sub(r\"where's\", \"where is\", res)\n","    res = re.sub(r\"how's\", \"how is\", res)\n","    res = re.sub(r\"\\'ll\", \" will\", res)\n","    res = re.sub(r\"\\'ve\", \" have\", res)\n","    res = re.sub(r\"\\'re\", \" are\", res)\n","    res = re.sub(r\"\\'d\", \" would\", res)\n","    res = re.sub(r\"\\'re\", \" are\", res)\n","    res = re.sub(r\"won't\", \"will not\", res)\n","    res = re.sub(r\"can't\", \"cannot\", res)\n","    res = re.sub(r\"n't\", \" not\", res)\n","    res = re.sub(r\"n'\", \"ng\", res)\n","    res = re.sub(r\"'bout\", \"about\", res)\n","    res = re.sub(r\"'til\", \"until\", res)\n","    res = re.sub(r\"[-()\\\"#/@;:<>{}`+=~|.!?,]\", \"\", res)\n","    \n","    # return the clean text\n","    return res\n","\n","# create a function to group the lines into conversations\n","def get_conversation_dict(sorted_chats):\n","  \n","    # create a conversation dictionary to store the index and dialouge\n","    conversation_dict = {}\n","    \n","    # create a temporary counter\n","    counter = 1\n","\n","    # store all index to one list\n","    conversation_ids = []\n","\n","    # iterate through all sorted conversations\n","    for i in range(1, len(sorted_chats) + 1):\n","\n","        # for all conversations index range between 1 to len(sorted_chats)\n","        if i < len(sorted_chats):\n","\n","            # if the current line number differs from the previous only by 1\n","            if (sorted_chats[i][0] - sorted_chats[i - 1][0]) == 1:\n","              \n","                # then this line is a part of the current conversation\n","                # if the previous line was not added before,\n","                # then we should add it now\n","                if sorted_chats[i - 1][1] not in conversation_ids:\n","                    conversation_ids.append(sorted_chats[i - 1][1])\n","                \n","                # or just append the current line\n","                conversation_ids.append(sorted_chats[i][1])\n","                \n","            # If the difference is more than 1\n","            # it means new conversation has started and we should clear conversation_ids\n","            elif (sorted_chats[i][0] - sorted_chats[i - 1][0]) > 1:\n","                conversation_dict[counter] = conversation_ids\n","                conversation_ids = []\n","                counter += 1\n","            else:\n","                continue\n","\n","    # return conversation dictionary with all conversations   \n","    return conversation_dict\n","\n","# create a function to prepare the list of questions and answers\n","def get_clean_q_and_a(conversations_dictionary):\n","\n","    # Create an questions and answers list\n","    questions_and_answer = []\n","    \n","    # iterate through each conversation\n","    for current_conversation in conversations_dictionary.values():\n","      \n","        # make sure that each conversation contains an even number of lines\n","        if len(current_conversation) % 2 != 0:\n","            current_conversation = current_conversation[:-1]\n","\n","        # convert questions and answers to the list of tuples\n","        for i in range(0, len(current_conversation), 2):\n","            questions_and_answer.append((current_conversation[i], current_conversation[i + 1]))\n","\n","    # zip with * operator unzips tuples into independent lists\n","    questions, answers = zip(*questions_and_answer)\n","    \n","    # get the list of the questions\n","    questions_list = list(questions)\n","\n","    # clear questions from contracted forms, non-letter symbols and convert it to lowercase\n","    clean_questions = list()\n","      \n","    for i in range(len(questions_list)):\n","        clean_questions.append(clean_text(questions_list[i]))\n","\n","    # get the list of the answers\n","    answer_list = list(answers)\n","\n","    # do the same with the answers, but now we need to add 'start' and 'end' words\n","    clean_answers = list()\n","    \n","    for i in range(len(answer_list)):\n","        clean_answers.append('<START> ' + clean_text(answer_list[i]) + ' <END>')\n","    \n","    # return clean answers and clean questions\n","    return clean_questions, clean_answers\n","\n","# run all the function to check for progress\n","\n","# get all the conversation from dataset\n","conversations = get_all_conversations()\n","\n","# get the total conversation length\n","total = len(conversations)\n","# print(\"Total conversations in dataset: {}\".format(total))\n","\n","# get all the sorted conversation\n","all_sorted_chats = get_all_sorted_chats(conversations)\n","\n","# get the conversation dictionary\n","conversation_dictionary = get_conversation_dict(all_sorted_chats)\n","\n","# get the list of questions and answers\n","questions, answers = get_clean_q_and_a(conversation_dictionary)\n","\n","# print total number of questions and answers\n","# print(\"Questions in dataset: {}\".format(len(questions)))\n","# print(\"Answers in dataset: {}\".format(len(answers)))"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"tyRLbGqHfwmi","executionInfo":{"status":"ok","timestamp":1601150074228,"user_tz":-330,"elapsed":3128,"user":{"displayName":"Sanket Sonje","photoUrl":"","userId":"05822068833245189120"}}},"source":["# load the model and keras hidden layes\n","# load the model\n","model = keras.models.load_model('/content/drive/My Drive/2020_Intern_03_VIIT_03_Chatbot/Coding Part (Implementation)/Seq2Seq Transformer/Flask/Model Data/model_2.h5')\n","    \n","# load all the layers\n","enc_inputs = model.layers[0].output\n","dec_inputs = model.layers[1].output\n","dec_embedding = model.layers[3].output\n","enc_outputs, state_h, state_c = model.layers[4].output\n","enc_states = [state_h, state_c]\n","dec_dense = model.get_layer(\"dense\")\n","dec_lstm = model.get_layer(\"lstm_1\")"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"IxRrYJZ5f4PX","executionInfo":{"status":"ok","timestamp":1601150074920,"user_tz":-330,"elapsed":2443,"user":{"displayName":"Sanket Sonje","photoUrl":"","userId":"05822068833245189120"}}},"source":["# create an inference model encoder\n","def make_inference_models():\n","    \n","    # two inputs for the state vectors returned by encoder\n","    dec_state_input_h = Input(shape=(200,),name=\"input_dec_state_h\")\n","    dec_state_input_c = Input(shape=(200,),name=\"input_dec_state_c\")\n","    dec_states_inputs = [dec_state_input_h, dec_state_input_c]\n","      \n","    # these state vectors are used as an initial state \n","    # for LSTM layer in the inference decoder\n","    # third input is the Embedding layer as explained above   \n","    dec_outputs, state_h, state_c = dec_lstm(dec_embedding, initial_state=dec_states_inputs)\n","    dec_states = [state_h, state_c]\n","      \n","    # Dense layer is used to return OHE predicted word\n","    dec_outputs = dec_dense(dec_outputs)\n","    dec_model = Model(inputs=[dec_inputs] + dec_states_inputs, outputs=[dec_outputs] + dec_states)\n","    \n","    # single encoder input is a question, represented as a sequence \n","    # of integers padded with zeros\n","    enc_model = Model(inputs=enc_inputs, outputs=enc_states)\n","    \n","    return enc_model, dec_model\n","\n","# run the above function to get the encoding and decoding sequence\n","enc_model, dec_model = make_inference_models()\n","\n","# main regular expression\n","target_regex = '!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~\\t\\n\\'0123456789'\n","\n","# Tokenizer allows to vectorize our corpus by turning each sentence into a sequence of integers where each integer is an index\n","# of a token in an internal dictionary\n","tokenizer = Tokenizer(filters=target_regex)\n","tokenizer.fit_on_texts(questions + answers)\n","\n","# get the vocab size\n","VOCAB_SIZE = len(tokenizer.word_index) + 1\n","# print('Vocabulary size : {}'.format(VOCAB_SIZE))\n","\n","# Create a function to convert the string into tokens\n","def str_to_tokens(sentence: str):\n","    # convert input string to lowercase, then split it by whitespaces\n","    words = sentence.lower().split()\n","      \n","    # then convert to a sequence of integers padded with zeros\n","    tokens_list = list()\n","    for current_word in words:\n","        result = tokenizer.word_index.get(current_word, '')\n","\n","        # if list is not empty then append the result into token_list\n","        if result != '':\n","            tokens_list.append(result)\n","\n","    # return One Hot Encodding of input string\n","    return pad_sequences([tokens_list], maxlen=maxlen_questions, padding='post')\n","\n","# tokenized and add padding to questions\n","tokenized_questions = tokenizer.texts_to_sequences(questions)\n","maxlen_questions = max([len(x) for x in tokenized_questions])\n","\n","# tokenized and add padding to questions\n","tokenized_answers = tokenizer.texts_to_sequences(answers)\n","maxlen_answers = max([len(x) for x in tokenized_answers])\n","\n","def Predictions(inputText):\n","    # main chatbot questions and answers\n","    # encode the input sequence into state vectors\n","    input_query = inputText\n","    # input_query = input('\\nEnter question : ')\n","\n","    # to continue the conversation\n","    states_values = enc_model.predict(str_to_tokens(input_query))\n","\n","    # start with a target sequence of size 1 - word 'start'   \n","    empty_target_seq = np.zeros((1, 1))\n","    empty_target_seq[0, 0] = tokenizer.word_index['start']\n","    stop_condition = False\n","    decoded_translation = ''\n","\n","    # loop until true to apply text generation algorithm\n","    while not stop_condition:\n","              \n","        # feed the state vectors and 1-word target sequence to the decoder to produce predictions for the next word\n","        dec_outputs, h, c = dec_model.predict([empty_target_seq] + states_values)         \n","                \n","        # sample the next word using these predictions\n","        sampled_word_index = np.argmax(dec_outputs[0, -1, :])\n","        sampled_word = None\n","                \n","        # append the sampled word to the target sequence\n","        for word, index in tokenizer.word_index.items():\n","            if sampled_word_index == index:\n","                if word != 'end':\n","                    decoded_translation += ' {}'.format(word)\n","                sampled_word = word\n","                \n","        # repeat until we generate the end-of-sequence word 'end' or we hit the length of answer limit\n","        if sampled_word == 'end' or len(decoded_translation.split()) > maxlen_answers:\n","            stop_condition = True\n","                \n","        # prepare next iteration\n","        empty_target_seq = np.zeros((1, 1))\n","        empty_target_seq[0, 0] = sampled_word_index\n","        states_values = [h, c]\n","          \n","    # print(\"Chatbot        :\",decoded_translation)\n","    return decoded_translation"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"e6fo8JhdeqTk","executionInfo":{"status":"ok","timestamp":1601150104958,"user_tz":-330,"elapsed":1230,"user":{"displayName":"Sanket Sonje","photoUrl":"","userId":"05822068833245189120"}},"outputId":"a8053fef-bd5b-4b95-9105-cfa38873d42f","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# testing for pickle data\n","print(Predictions(\"who\"))"],"execution_count":9,"outputs":[{"output_type":"stream","text":[" bianca\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"H8fU1v5Qesbl"},"source":["# start the flask app\n","app = Flask(__name__)\n","\n","# Handle the api and send the prediction to the user\n","@app.route('/', methods = ['POST'])\n","def response():\n","\n","    # Get the information send by andriod\n","    req = request.json.get()\n","    input_query = req['userQuery']\n","\n","    # Predict the output\n","    output_response = Prediction(input_query)\n","\n","    # Store the output in req[userQuery]\n","    req['userQuery'] = output_response\n","    \n","    # return the json file with output response\n","    return req\n","\n","if __name__ == '__main__':\n","\t  app.run(host=\"127.0.0.1\")"],"execution_count":null,"outputs":[]}]}