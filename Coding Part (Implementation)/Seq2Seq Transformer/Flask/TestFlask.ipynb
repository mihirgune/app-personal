{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"TestFlask.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPMKKeXrtNOqSNxZv6EhjIF"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"fJxf8HyMF18e","executionInfo":{"status":"ok","timestamp":1601149181743,"user_tz":-330,"elapsed":1115,"user":{"displayName":"Sanket Sonje","photoUrl":"","userId":"05822068833245189120"}},"outputId":"0377af4e-cb8f-45ef-a292-73a71aa04cca","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"i9VVGVWBGmHO","executionInfo":{"status":"ok","timestamp":1601149183837,"user_tz":-330,"elapsed":3191,"user":{"displayName":"Sanket Sonje","photoUrl":"","userId":"05822068833245189120"}}},"source":["# import all the necessary libraries\n","import codecs\n","import io\n","import os\n","import re\n","import numpy as np\n","import requests\n","from gensim.models import Word2Vec\n","from keras import Input, Model\n","from keras.activations import softmax\n","from keras.layers import Embedding, LSTM, Dense\n","from keras.optimizers import RMSprop\n","from keras.preprocessing.sequence import pad_sequences\n","from keras.utils import to_categorical\n","from keras_preprocessing.text import Tokenizer"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"alR6orfxHv1J","executionInfo":{"status":"ok","timestamp":1601149183839,"user_tz":-330,"elapsed":2877,"user":{"displayName":"Sanket Sonje","photoUrl":"","userId":"05822068833245189120"}}},"source":["# Get the data from file\n","def get_all_conversations():\n","  all_conversations = []\n","  with codecs.open(\"/content/drive/My Drive/2020_Intern_03_VIIT_03_Chatbot/Coding Part (Implementation)/Seq2Seq Transformer/Cornell Movie Dataset/cornell movie-dialogs corpus/movie_lines.txt\", \"rb\", encoding=\"utf-8\", errors=\"ignore\") as f:\n","  \n","    # split corpus line line\n","    lines = f.read().split(\"\\n\")\n","    \n","    # get each conversation\n","    for line in lines:\n","    \n","      # each line has multiple columns divided by '+++$+++'\n","      all_conversations.append(line.split(\" +++$+++ \"))\n","  \n","  # return all conversation\n","  return all_conversations"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZFQSJ_bqHwkC","executionInfo":{"status":"ok","timestamp":1601149183840,"user_tz":-330,"elapsed":1805,"user":{"displayName":"Sanket Sonje","photoUrl":"","userId":"05822068833245189120"}}},"source":["# Dataset is too big hence taking only first 10000 lines\n","# create a function to get all sorted conversation\n","def get_all_sorted_chats(all_conversations):\n","    all_chats = {}\n","    for tokens in all_conversations[:2000]:\n","\n","        # if the line is valid - it contains all the metadata\n","        if len(tokens) > 4:\n","\n","            # save the line number and the text itself\n","            # 4 th is the index where actual dialogue is present    \n","            all_chats[int(tokens[0][1:])] = tokens[4]\n","\n","    # then sort the result and return list of tuples\n","    return sorted(all_chats.items(), key=lambda x: x[0])"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"pTHzN3LeHvrl","executionInfo":{"status":"ok","timestamp":1601149185117,"user_tz":-330,"elapsed":728,"user":{"displayName":"Sanket Sonje","photoUrl":"","userId":"05822068833245189120"}}},"source":["# create a function to clean the text\n","def clean_text(text_to_clean):\n","\n","  # apply all these conditions to clean the text\n","  res = text_to_clean.lower()\n","  res = re.sub(r\"i'm\", \"i am\", res)\n","  res = re.sub(r\"he's\", \"he is\", res)\n","  res = re.sub(r\"she's\", \"she is\", res)\n","  res = re.sub(r\"it's\", \"it is\", res)\n","  res = re.sub(r\"that's\", \"that is\", res)\n","  res = re.sub(r\"what's\", \"what is\", res)\n","  res = re.sub(r\"where's\", \"where is\", res)\n","  res = re.sub(r\"how's\", \"how is\", res)\n","  res = re.sub(r\"\\'ll\", \" will\", res)\n","  res = re.sub(r\"\\'ve\", \" have\", res)\n","  res = re.sub(r\"\\'re\", \" are\", res)\n","  res = re.sub(r\"\\'d\", \" would\", res)\n","  res = re.sub(r\"\\'re\", \" are\", res)\n","  res = re.sub(r\"won't\", \"will not\", res)\n","  res = re.sub(r\"can't\", \"cannot\", res)\n","  res = re.sub(r\"n't\", \" not\", res)\n","  res = re.sub(r\"n'\", \"ng\", res)\n","  res = re.sub(r\"'bout\", \"about\", res)\n","  res = re.sub(r\"'til\", \"until\", res)\n","  res = re.sub(r\"[-()\\\"#/@;:<>{}`+=~|.!?,]\", \"\", res)\n","  \n","  # return the clean text\n","  return res"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"_V1qCh7NH3j_","executionInfo":{"status":"ok","timestamp":1601149187998,"user_tz":-330,"elapsed":890,"user":{"displayName":"Sanket Sonje","photoUrl":"","userId":"05822068833245189120"}}},"source":["# create a function to group the lines into conversations\n","def get_conversation_dict(sorted_chats):\n","  \n","  # create a conversation dictionary to store the index and dialouge\n","  conversation_dict = {}\n","  \n","  # create a temporary counter\n","  counter = 1\n","\n","  # store all index to one list\n","  conversation_ids = []\n","\n","  # iterate through all sorted conversations\n","  for i in range(1, len(sorted_chats) + 1):\n","\n","    # for all conversations index range between 1 to len(sorted_chats)\n","    if i < len(sorted_chats):\n","\n","      # if the current line number differs from the previous only by 1\n","      if (sorted_chats[i][0] - sorted_chats[i - 1][0]) == 1:\n","        \n","        # then this line is a part of the current conversation\n","        # if the previous line was not added before,\n","        # then we should add it now\n","        if sorted_chats[i - 1][1] not in conversation_ids:\n","          conversation_ids.append(sorted_chats[i - 1][1])\n","        \n","        # or just append the current line\n","        conversation_ids.append(sorted_chats[i][1])\n","            \n","      # If the difference is more than 1\n","      # it means new conversation has started and we should clear conversation_ids\n","      elif (sorted_chats[i][0] - sorted_chats[i - 1][0]) > 1:\n","        conversation_dict[counter] = conversation_ids\n","        conversation_ids = []\n","        counter += 1\n","      else:\n","        continue\n","\n","  # return conversation dictionary with all conversations   \n","  return conversation_dict"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"6OMCMzOWH3S4","executionInfo":{"status":"ok","timestamp":1601149190831,"user_tz":-330,"elapsed":1171,"user":{"displayName":"Sanket Sonje","photoUrl":"","userId":"05822068833245189120"}}},"source":["# create a function to prepare the list of questions and answers\n","def get_clean_q_and_a(conversations_dictionary):\n","\n","  # Create an questions and answers list\n","  questions_and_answer = []\n","  \n","  # iterate through each conversation\n","  for current_conversation in conversations_dictionary.values():\n","    \n","    # make sure that each conversation contains an even number of lines\n","    if len(current_conversation) % 2 != 0:\n","      current_conversation = current_conversation[:-1]\n","\n","    # convert questions and answers to the list of tuples\n","    for i in range(0, len(current_conversation), 2):\n","      questions_and_answer.append((current_conversation[i], current_conversation[i + 1]))\n","\n","  # zip with * operator unzips tuples into independent lists\n","  questions, answers = zip(*questions_and_answer)\n","  \n","  # get the list of the questions\n","  questions_list = list(questions)\n","\n","  # clear questions from contracted forms, non-letter symbols and convert it to lowercase\n","  clean_questions = list()\n","    \n","  for i in range(len(questions_list)):\n","    clean_questions.append(clean_text(questions_list[i]))\n","\n","  # get the list of the answers\n","  answer_list = list(answers)\n","\n","  # do the same with the answers, but now we need to add 'start' and 'end' words\n","  clean_answers = list()\n","  \n","  for i in range(len(answer_list)):\n","    clean_answers.append('<START> ' + clean_text(answer_list[i]) + ' <END>')\n","  \n","  # return clean answers and clean questions\n","  return clean_questions, clean_answers"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"ovzOcx3AH3J9","executionInfo":{"status":"ok","timestamp":1601149193888,"user_tz":-330,"elapsed":1835,"user":{"displayName":"Sanket Sonje","photoUrl":"","userId":"05822068833245189120"}},"outputId":"02b66687-6f5b-4c5b-f6f9-cd09fb9023fc","colab":{"base_uri":"https://localhost:8080/","height":68}},"source":["# run all the function to check for progress\n","\n","# get all the conversation from dataset\n","conversations = get_all_conversations()\n","\n","# get the total conversation length\n","total = len(conversations)\n","print(\"Total conversations in dataset: {}\".format(total))\n","\n","# get all the sorted conversation\n","all_sorted_chats = get_all_sorted_chats(conversations)\n","\n","# get the conversation dictionary\n","conversation_dictionary = get_conversation_dict(all_sorted_chats)\n","\n","# get the list of questions and answers\n","questions, answers = get_clean_q_and_a(conversation_dictionary)\n","\n","# print total number of questions and answers\n","print(\"Questions in dataset: {}\".format(len(questions)))\n","print(\"Answers in dataset: {}\".format(len(answers)))"],"execution_count":8,"outputs":[{"output_type":"stream","text":["Total conversations in dataset: 304714\n","Questions in dataset: 928\n","Answers in dataset: 928\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"kCRjXI7gH8TD","executionInfo":{"status":"ok","timestamp":1601149194959,"user_tz":-330,"elapsed":1171,"user":{"displayName":"Sanket Sonje","photoUrl":"","userId":"05822068833245189120"}},"outputId":"b5c1f99f-aecd-4c04-efd6-8425f4f8f5f8","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# main regular expression\n","target_regex = '!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~\\t\\n\\'0123456789'\n","\n","# Tokenizer allows to vectorize our corpus by turning each sentence into a sequence of integers where each integer is an index\n","# of a token in an internal dictionary\n","tokenizer = Tokenizer(filters=target_regex)\n","tokenizer.fit_on_texts(questions + answers)\n","\n","# get the vocab size\n","VOCAB_SIZE = len(tokenizer.word_index) + 1\n","print('Vocabulary size : {}'.format(VOCAB_SIZE))"],"execution_count":9,"outputs":[{"output_type":"stream","text":["Vocabulary size : 2664\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"kmRJe5siH8w2","executionInfo":{"status":"ok","timestamp":1601149197099,"user_tz":-330,"elapsed":926,"user":{"displayName":"Sanket Sonje","photoUrl":"","userId":"05822068833245189120"}},"outputId":"b8108089-d627-4fed-a229-40fc8554f920","colab":{"base_uri":"https://localhost:8080/","height":119}},"source":["# Prepare two matrices from the lists of questions and answers\n","# use OHE for padding\n","\n","# tokenized and add padding to questions\n","tokenized_questions = tokenizer.texts_to_sequences(questions)\n","maxlen_questions = max([len(x) for x in tokenized_questions])\n","print(\"\\nMax length of questions is :- \",maxlen_questions)\n","\n","# pad each question with zeros at the end to be 223 words long\n","encoder_input_data = pad_sequences(tokenized_questions, maxlen=maxlen_questions, padding='post')\n","\n","# matrix of 4709x223 integers - 4709 questions 223 words each\n","print(encoder_input_data.shape)\n","\n","# tokenized and add padding to questions\n","tokenized_answers = tokenizer.texts_to_sequences(answers)\n","maxlen_answers = max([len(x) for x in tokenized_answers])\n","print(\"\\nMax length of answers is :- \",maxlen_answers)\n","\n","# pad each answer with zeros at the end to be 132 words long\n","decoder_input_data = pad_sequences(tokenized_answers, maxlen=maxlen_answers, padding='post')\n","\n","# matrix of 4709x132 integers - 4709 answers 132 words each\n","print(decoder_input_data.shape)"],"execution_count":10,"outputs":[{"output_type":"stream","text":["\n","Max length of questions is :-  106\n","(928, 106)\n","\n","Max length of answers is :-  132\n","(928, 132)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"8ZeW8rsBH8Wx","executionInfo":{"status":"ok","timestamp":1601149199493,"user_tz":-330,"elapsed":1932,"user":{"displayName":"Sanket Sonje","photoUrl":"","userId":"05822068833245189120"}}},"source":["# Create one-hot encoded answers\n","\n","# remove the first 'start' word from every answer\n","for i in range(len(tokenized_answers)):\n","  tokenized_answers[i] = tokenized_answers[i][1:]\n","\n","# pad answers with zeros\n","padded_answers = pad_sequences(tokenized_answers, maxlen=maxlen_answers, padding='post')\n","\n","# tensor of size (4709, 132, 7910)\n","# 4709 answers 132 words each, and each word is one-hot encoded using our vocabulary\n","decoder_output_data = to_categorical(padded_answers, VOCAB_SIZE)"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"Bn0kHdaPH7-D","executionInfo":{"status":"ok","timestamp":1601149254406,"user_tz":-330,"elapsed":2662,"user":{"displayName":"Sanket Sonje","photoUrl":"","userId":"05822068833245189120"}},"outputId":"2d476dd6-07b1-4523-f1b1-1b515a142839","colab":{"base_uri":"https://localhost:8080/","height":425}},"source":["# encoder will be used to capture space-dependent \n","# relations between words from the questions\n","enc_inputs = Input(shape=(None,), name=\"E_input_1\")\n","enc_embedding = Embedding(VOCAB_SIZE, 200, mask_zero=True, name=\"E_embedding_1\")(enc_inputs)\n","enc_outputs, state_h, state_c = LSTM(200, return_state=True, name=\"E_lstm_1\")(enc_embedding)\n","enc_states = [state_h, state_c]\n","\n","# decoder will be used to capture space-dependent relations between words from the answers using encoder's internal state as a context\n","dec_inputs = Input(shape=(None,), name=\"D_input_1\")\n","dec_embedding = Embedding(VOCAB_SIZE, 200, mask_zero=True, name=\"D_embedding_1\")(dec_inputs)\n","dec_lstm = LSTM(200, return_state=True, return_sequences=True, name=\"D_lstm_1\")\n","dec_outputs, _, _ = dec_lstm(dec_embedding, initial_state=enc_states)\n","\n","# decoder is connected to the output Dense layer\n","dec_dense = Dense(VOCAB_SIZE, activation=softmax, name=\"Dense_1\")\n","output = dec_dense(dec_outputs)\n","model = Model([enc_inputs, dec_inputs], output)\n","\n","# output of this network will look like this:\n","# y_true = [0.05, 0.95, 0...]\n","# and expected one-hot encoded output like this:\n","# y_pred = [0, 1, 0...]\n","model.compile(optimizer=RMSprop(), loss='categorical_crossentropy')\n","model.summary()"],"execution_count":14,"outputs":[{"output_type":"stream","text":["Model: \"functional_4\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","E_input_1 (InputLayer)          [(None, None)]       0                                            \n","__________________________________________________________________________________________________\n","D_input_1 (InputLayer)          [(None, None)]       0                                            \n","__________________________________________________________________________________________________\n","E_embedding_1 (Embedding)       (None, None, 200)    532800      E_input_1[0][0]                  \n","__________________________________________________________________________________________________\n","D_embedding_1 (Embedding)       (None, None, 200)    532800      D_input_1[0][0]                  \n","__________________________________________________________________________________________________\n","E_lstm_1 (LSTM)                 [(None, 200), (None, 320800      E_embedding_1[0][0]              \n","__________________________________________________________________________________________________\n","D_lstm_1 (LSTM)                 [(None, None, 200),  320800      D_embedding_1[0][0]              \n","                                                                 E_lstm_1[0][1]                   \n","                                                                 E_lstm_1[0][2]                   \n","__________________________________________________________________________________________________\n","Dense_1 (Dense)                 (None, None, 2664)   535464      D_lstm_1[0][0]                   \n","==================================================================================================\n","Total params: 2,242,664\n","Trainable params: 2,242,664\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"EGfozJ0SH7z8","executionInfo":{"status":"ok","timestamp":1601147318081,"user_tz":-330,"elapsed":2742593,"user":{"displayName":"Sanket Sonje","photoUrl":"","userId":"05822068833245189120"}},"outputId":"7b477fa5-9719-48de-a6a7-5622a1dd59d2","colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["model.fit([encoder_input_data, decoder_input_data], decoder_output_data, batch_size=50, epochs=300)"],"execution_count":14,"outputs":[{"output_type":"stream","text":["Epoch 1/150\n","19/19 [==============================] - 33s 2s/step - loss: 0.4347\n","Epoch 2/150\n","19/19 [==============================] - 33s 2s/step - loss: 0.4278\n","Epoch 3/150\n","19/19 [==============================] - 33s 2s/step - loss: 0.4221\n","Epoch 4/150\n","19/19 [==============================] - 33s 2s/step - loss: 0.4169\n","Epoch 5/150\n","19/19 [==============================] - 33s 2s/step - loss: 0.4117\n","Epoch 6/150\n","19/19 [==============================] - 33s 2s/step - loss: 0.4069\n","Epoch 7/150\n","19/19 [==============================] - 33s 2s/step - loss: 0.4022\n","Epoch 8/150\n","19/19 [==============================] - 33s 2s/step - loss: 0.3975\n","Epoch 9/150\n","19/19 [==============================] - 33s 2s/step - loss: 0.3929\n","Epoch 10/150\n","19/19 [==============================] - 33s 2s/step - loss: 0.3883\n","Epoch 11/150\n","19/19 [==============================] - 33s 2s/step - loss: 0.3836\n","Epoch 12/150\n","19/19 [==============================] - 36s 2s/step - loss: 0.3787\n","Epoch 13/150\n","19/19 [==============================] - 33s 2s/step - loss: 0.3743\n","Epoch 14/150\n","19/19 [==============================] - 33s 2s/step - loss: 0.3696\n","Epoch 15/150\n","19/19 [==============================] - 33s 2s/step - loss: 0.3653\n","Epoch 16/150\n","19/19 [==============================] - 33s 2s/step - loss: 0.3604\n","Epoch 17/150\n","19/19 [==============================] - 33s 2s/step - loss: 0.3558\n","Epoch 18/150\n","19/19 [==============================] - 33s 2s/step - loss: 0.3510\n","Epoch 19/150\n","19/19 [==============================] - 33s 2s/step - loss: 0.3462\n","Epoch 20/150\n","19/19 [==============================] - 33s 2s/step - loss: 0.3417\n","Epoch 21/150\n","19/19 [==============================] - 33s 2s/step - loss: 0.3371\n","Epoch 22/150\n","19/19 [==============================] - 33s 2s/step - loss: 0.3324\n","Epoch 23/150\n","19/19 [==============================] - 33s 2s/step - loss: 0.3278\n","Epoch 24/150\n","19/19 [==============================] - 33s 2s/step - loss: 0.3235\n","Epoch 25/150\n","19/19 [==============================] - 33s 2s/step - loss: 0.3189\n","Epoch 26/150\n","19/19 [==============================] - 33s 2s/step - loss: 0.3145\n","Epoch 27/150\n","19/19 [==============================] - 33s 2s/step - loss: 0.3101\n","Epoch 28/150\n","19/19 [==============================] - 33s 2s/step - loss: 0.3057\n","Epoch 29/150\n","19/19 [==============================] - 36s 2s/step - loss: 0.3014\n","Epoch 30/150\n","19/19 [==============================] - 33s 2s/step - loss: 0.2972\n","Epoch 31/150\n","19/19 [==============================] - 33s 2s/step - loss: 0.2927\n","Epoch 32/150\n","19/19 [==============================] - 33s 2s/step - loss: 0.2883\n","Epoch 33/150\n","19/19 [==============================] - 33s 2s/step - loss: 0.2842\n","Epoch 34/150\n","19/19 [==============================] - 33s 2s/step - loss: 0.2798\n","Epoch 35/150\n","19/19 [==============================] - 34s 2s/step - loss: 0.2756\n","Epoch 36/150\n","19/19 [==============================] - 34s 2s/step - loss: 0.2716\n","Epoch 37/150\n","19/19 [==============================] - 33s 2s/step - loss: 0.2669\n","Epoch 38/150\n","19/19 [==============================] - 34s 2s/step - loss: 0.2630\n","Epoch 39/150\n","19/19 [==============================] - 33s 2s/step - loss: 0.2588\n","Epoch 40/150\n","19/19 [==============================] - 34s 2s/step - loss: 0.2541\n","Epoch 41/150\n","19/19 [==============================] - 33s 2s/step - loss: 0.2503\n","Epoch 42/150\n","19/19 [==============================] - 33s 2s/step - loss: 0.2460\n","Epoch 43/150\n","19/19 [==============================] - 33s 2s/step - loss: 0.2421\n","Epoch 44/150\n","19/19 [==============================] - 33s 2s/step - loss: 0.2371\n","Epoch 45/150\n","19/19 [==============================] - 33s 2s/step - loss: 0.2333\n","Epoch 46/150\n","19/19 [==============================] - 36s 2s/step - loss: 0.2291\n","Epoch 47/150\n","19/19 [==============================] - 33s 2s/step - loss: 0.2246\n","Epoch 48/150\n","19/19 [==============================] - 33s 2s/step - loss: 0.2205\n","Epoch 49/150\n","19/19 [==============================] - 34s 2s/step - loss: 0.2166\n","Epoch 50/150\n","19/19 [==============================] - 34s 2s/step - loss: 0.2124\n","Epoch 51/150\n","19/19 [==============================] - 34s 2s/step - loss: 0.2081\n","Epoch 52/150\n","19/19 [==============================] - 34s 2s/step - loss: 0.2038\n","Epoch 53/150\n","19/19 [==============================] - 34s 2s/step - loss: 0.2003\n","Epoch 54/150\n","19/19 [==============================] - 34s 2s/step - loss: 0.1962\n","Epoch 55/150\n","19/19 [==============================] - 33s 2s/step - loss: 0.1919\n","Epoch 56/150\n","19/19 [==============================] - 34s 2s/step - loss: 0.1878\n","Epoch 57/150\n","19/19 [==============================] - 33s 2s/step - loss: 0.1838\n","Epoch 58/150\n","19/19 [==============================] - 34s 2s/step - loss: 0.1804\n","Epoch 59/150\n","19/19 [==============================] - 33s 2s/step - loss: 0.1763\n","Epoch 60/150\n","19/19 [==============================] - 33s 2s/step - loss: 0.1724\n","Epoch 61/150\n","19/19 [==============================] - 33s 2s/step - loss: 0.1694\n","Epoch 62/150\n","19/19 [==============================] - 33s 2s/step - loss: 0.1644\n","Epoch 63/150\n","19/19 [==============================] - 36s 2s/step - loss: 0.1611\n","Epoch 64/150\n","19/19 [==============================] - 33s 2s/step - loss: 0.1576\n","Epoch 65/150\n","19/19 [==============================] - 33s 2s/step - loss: 0.1541\n","Epoch 66/150\n","19/19 [==============================] - 33s 2s/step - loss: 0.1505\n","Epoch 67/150\n","19/19 [==============================] - 33s 2s/step - loss: 0.1460\n","Epoch 68/150\n","19/19 [==============================] - 34s 2s/step - loss: 0.1436\n","Epoch 69/150\n","19/19 [==============================] - 33s 2s/step - loss: 0.1394\n","Epoch 70/150\n","19/19 [==============================] - 34s 2s/step - loss: 0.1368\n","Epoch 71/150\n","19/19 [==============================] - 33s 2s/step - loss: 0.1323\n","Epoch 72/150\n","19/19 [==============================] - 33s 2s/step - loss: 0.1295\n","Epoch 73/150\n","19/19 [==============================] - 34s 2s/step - loss: 0.1261\n","Epoch 74/150\n","19/19 [==============================] - 34s 2s/step - loss: 0.1228\n","Epoch 75/150\n","19/19 [==============================] - 34s 2s/step - loss: 0.1196\n","Epoch 76/150\n","19/19 [==============================] - 34s 2s/step - loss: 0.1168\n","Epoch 77/150\n","19/19 [==============================] - 33s 2s/step - loss: 0.1130\n","Epoch 78/150\n","19/19 [==============================] - 33s 2s/step - loss: 0.1107\n","Epoch 79/150\n","19/19 [==============================] - 34s 2s/step - loss: 0.1073\n","Epoch 80/150\n","19/19 [==============================] - 37s 2s/step - loss: 0.1039\n","Epoch 81/150\n","19/19 [==============================] - 34s 2s/step - loss: 0.1022\n","Epoch 82/150\n","19/19 [==============================] - 33s 2s/step - loss: 0.0983\n","Epoch 83/150\n","19/19 [==============================] - 33s 2s/step - loss: 0.0957\n","Epoch 84/150\n","19/19 [==============================] - 34s 2s/step - loss: 0.0927\n","Epoch 85/150\n","19/19 [==============================] - 33s 2s/step - loss: 0.0911\n","Epoch 86/150\n","19/19 [==============================] - 34s 2s/step - loss: 0.0873\n","Epoch 87/150\n","19/19 [==============================] - 34s 2s/step - loss: 0.0852\n","Epoch 88/150\n","19/19 [==============================] - 33s 2s/step - loss: 0.0821\n","Epoch 89/150\n","19/19 [==============================] - 33s 2s/step - loss: 0.0799\n","Epoch 90/150\n","19/19 [==============================] - 33s 2s/step - loss: 0.0778\n","Epoch 91/150\n","19/19 [==============================] - 34s 2s/step - loss: 0.0749\n","Epoch 92/150\n","19/19 [==============================] - 33s 2s/step - loss: 0.0724\n","Epoch 93/150\n","19/19 [==============================] - 34s 2s/step - loss: 0.0709\n","Epoch 94/150\n","19/19 [==============================] - 33s 2s/step - loss: 0.0682\n","Epoch 95/150\n","19/19 [==============================] - 33s 2s/step - loss: 0.0656\n","Epoch 96/150\n","19/19 [==============================] - 34s 2s/step - loss: 0.0641\n","Epoch 97/150\n","19/19 [==============================] - 35s 2s/step - loss: 0.0612\n","Epoch 98/150\n","19/19 [==============================] - 34s 2s/step - loss: 0.0592\n","Epoch 99/150\n","19/19 [==============================] - 34s 2s/step - loss: 0.0574\n","Epoch 100/150\n","19/19 [==============================] - 33s 2s/step - loss: 0.0552\n","Epoch 101/150\n","19/19 [==============================] - 33s 2s/step - loss: 0.0536\n","Epoch 102/150\n","19/19 [==============================] - 33s 2s/step - loss: 0.0520\n","Epoch 103/150\n","19/19 [==============================] - 33s 2s/step - loss: 0.0494\n","Epoch 104/150\n","19/19 [==============================] - 34s 2s/step - loss: 0.0480\n","Epoch 105/150\n","19/19 [==============================] - 33s 2s/step - loss: 0.0465\n","Epoch 106/150\n","19/19 [==============================] - 33s 2s/step - loss: 0.0445\n","Epoch 107/150\n","19/19 [==============================] - 33s 2s/step - loss: 0.0428\n","Epoch 108/150\n","19/19 [==============================] - 34s 2s/step - loss: 0.0415\n","Epoch 109/150\n","19/19 [==============================] - 34s 2s/step - loss: 0.0398\n","Epoch 110/150\n","19/19 [==============================] - 34s 2s/step - loss: 0.0384\n","Epoch 111/150\n","19/19 [==============================] - 34s 2s/step - loss: 0.0364\n","Epoch 112/150\n","19/19 [==============================] - 34s 2s/step - loss: 0.0354\n","Epoch 113/150\n","19/19 [==============================] - 34s 2s/step - loss: 0.0340\n","Epoch 114/150\n","19/19 [==============================] - 34s 2s/step - loss: 0.0325\n","Epoch 115/150\n","19/19 [==============================] - 36s 2s/step - loss: 0.0316\n","Epoch 116/150\n","19/19 [==============================] - 34s 2s/step - loss: 0.0300\n","Epoch 117/150\n","19/19 [==============================] - 34s 2s/step - loss: 0.0288\n","Epoch 118/150\n","19/19 [==============================] - 34s 2s/step - loss: 0.0277\n","Epoch 119/150\n","19/19 [==============================] - 34s 2s/step - loss: 0.0271\n","Epoch 120/150\n","19/19 [==============================] - 34s 2s/step - loss: 0.0252\n","Epoch 121/150\n","19/19 [==============================] - 34s 2s/step - loss: 0.0242\n","Epoch 122/150\n","19/19 [==============================] - 34s 2s/step - loss: 0.0232\n","Epoch 123/150\n","19/19 [==============================] - 33s 2s/step - loss: 0.0223\n","Epoch 124/150\n","19/19 [==============================] - 33s 2s/step - loss: 0.0216\n","Epoch 125/150\n","19/19 [==============================] - 33s 2s/step - loss: 0.0201\n","Epoch 126/150\n","19/19 [==============================] - 33s 2s/step - loss: 0.0195\n","Epoch 127/150\n","19/19 [==============================] - 33s 2s/step - loss: 0.0188\n","Epoch 128/150\n","19/19 [==============================] - 33s 2s/step - loss: 0.0179\n","Epoch 129/150\n","19/19 [==============================] - 33s 2s/step - loss: 0.0168\n","Epoch 130/150\n","19/19 [==============================] - 34s 2s/step - loss: 0.0164\n","Epoch 131/150\n","19/19 [==============================] - 33s 2s/step - loss: 0.0154\n","Epoch 132/150\n","19/19 [==============================] - 36s 2s/step - loss: 0.0148\n","Epoch 133/150\n","19/19 [==============================] - 33s 2s/step - loss: 0.0141\n","Epoch 134/150\n","19/19 [==============================] - 33s 2s/step - loss: 0.0134\n","Epoch 135/150\n","19/19 [==============================] - 33s 2s/step - loss: 0.0130\n","Epoch 136/150\n","19/19 [==============================] - 34s 2s/step - loss: 0.0123\n","Epoch 137/150\n","19/19 [==============================] - 34s 2s/step - loss: 0.0117\n","Epoch 138/150\n","19/19 [==============================] - 33s 2s/step - loss: 0.0112\n","Epoch 139/150\n","19/19 [==============================] - 34s 2s/step - loss: 0.0107\n","Epoch 140/150\n","19/19 [==============================] - 34s 2s/step - loss: 0.0100\n","Epoch 141/150\n","19/19 [==============================] - 34s 2s/step - loss: 0.0095\n","Epoch 142/150\n","19/19 [==============================] - 34s 2s/step - loss: 0.0093\n","Epoch 143/150\n","19/19 [==============================] - 33s 2s/step - loss: 0.0087\n","Epoch 144/150\n","19/19 [==============================] - 33s 2s/step - loss: 0.0083\n","Epoch 145/150\n","19/19 [==============================] - 33s 2s/step - loss: 0.0079\n","Epoch 146/150\n","19/19 [==============================] - 34s 2s/step - loss: 0.0077\n","Epoch 147/150\n","19/19 [==============================] - 33s 2s/step - loss: 0.0071\n","Epoch 148/150\n","19/19 [==============================] - 33s 2s/step - loss: 0.0068\n","Epoch 149/150\n","19/19 [==============================] - 37s 2s/step - loss: 0.0065\n","Epoch 150/150\n","19/19 [==============================] - 33s 2s/step - loss: 0.0064\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<tensorflow.python.keras.callbacks.History at 0x7f7f30492d68>"]},"metadata":{"tags":[]},"execution_count":14}]},{"cell_type":"code","metadata":{"id":"dAyfmiPqGvAz","executionInfo":{"status":"ok","timestamp":1601147318082,"user_tz":-330,"elapsed":26,"user":{"displayName":"Sanket Sonje","photoUrl":"","userId":"05822068833245189120"}}},"source":["# save the model\n","model.save('model_2.h5')"],"execution_count":15,"outputs":[]},{"cell_type":"code","metadata":{"id":"L_eY6EOpGvxu","executionInfo":{"status":"ok","timestamp":1601148564565,"user_tz":-330,"elapsed":1773,"user":{"displayName":"Sanket Sonje","photoUrl":"","userId":"05822068833245189120"}}},"source":["# create an inference model encoder\n","def make_inference_models():\n","  # two inputs for the state vectors returned by encoder\n","  dec_state_input_h = Input(shape=(200,))\n","  dec_state_input_c = Input(shape=(200,))\n","  dec_states_inputs = [dec_state_input_h, dec_state_input_c]\n","    \n","  # these state vectors are used as an initial state \n","  # for LSTM layer in the inference decoder\n","  # third input is the Embedding layer as explained above   \n","  dec_outputs, state_h, state_c = dec_lstm(dec_embedding, initial_state=dec_states_inputs)\n","  dec_states = [state_h, state_c]\n","    \n","  # Dense layer is used to return OHE predicted word\n","  dec_outputs = dec_dense(dec_outputs)\n","  dec_model = Model(inputs=[dec_inputs] + dec_states_inputs, outputs=[dec_outputs] + dec_states)\n","   \n","  # single encoder input is a question, represented as a sequence \n","  # of integers padded with zeros\n","  enc_model = Model(inputs=enc_inputs, outputs=enc_states)\n","   \n","  return enc_model, dec_model\n","\n","# run the above function to get the encoding and decoding sequence\n","enc_model, dec_model = make_inference_models()"],"execution_count":26,"outputs":[]},{"cell_type":"code","metadata":{"id":"Jlf797eNGvu4","executionInfo":{"status":"ok","timestamp":1601147338295,"user_tz":-330,"elapsed":606,"user":{"displayName":"Sanket Sonje","photoUrl":"","userId":"05822068833245189120"}}},"source":["# Create a function to convert the string into tokens\n","def str_to_tokens(sentence: str):\n","  # convert input string to lowercase, then split it by whitespaces\n","  words = sentence.lower().split()\n","    \n","  # then convert to a sequence of integers padded with zeros\n","  tokens_list = list()\n","  for current_word in words:\n","    result = tokenizer.word_index.get(current_word, '')\n","\n","    # if list is not empty then append the result into token_list\n","    if result != '':\n","      tokens_list.append(result)\n","\n","  # return One Hot Encodding of input string\n","  return pad_sequences([tokens_list], maxlen=maxlen_questions, padding='post')"],"execution_count":17,"outputs":[]},{"cell_type":"code","metadata":{"id":"1JQNOYc2NAjL","executionInfo":{"status":"ok","timestamp":1601147341888,"user_tz":-330,"elapsed":1146,"user":{"displayName":"Sanket Sonje","photoUrl":"","userId":"05822068833245189120"}}},"source":["def Predictions(inputText):\n","  # main chatbot questions and answers\n","  # encode the input sequence into state vectors\n","  input_query = inputText\n","  # input_query = input('\\nEnter question : ')\n","\n","  # to continue the conversation\n","  states_values = enc_model.predict(str_to_tokens(input_query))\n","\n","  # start with a target sequence of size 1 - word 'start'   \n","  empty_target_seq = np.zeros((1, 1))\n","  empty_target_seq[0, 0] = tokenizer.word_index['start']\n","  stop_condition = False\n","  decoded_translation = ''\n","\n","  # loop until true to apply text generation algorithm\n","  while not stop_condition:\n","            \n","    # feed the state vectors and 1-word target sequence to the decoder to produce predictions for the next word\n","    dec_outputs, h, c = dec_model.predict([empty_target_seq] + states_values)         \n","            \n","    # sample the next word using these predictions\n","    sampled_word_index = np.argmax(dec_outputs[0, -1, :])\n","    sampled_word = None\n","            \n","    # append the sampled word to the target sequence\n","    for word, index in tokenizer.word_index.items():\n","      if sampled_word_index == index:\n","        if word != 'end':\n","          decoded_translation += ' {}'.format(word)\n","        sampled_word = word\n","            \n","    # repeat until we generate the end-of-sequence word 'end' or we hit the length of answer limit\n","    if sampled_word == 'end' or len(decoded_translation.split()) > maxlen_answers:\n","      stop_condition = True\n","            \n","    # prepare next iteration\n","    empty_target_seq = np.zeros((1, 1))\n","    empty_target_seq[0, 0] = sampled_word_index\n","    states_values = [h, c]\n","        \n","  # print(\"Chatbot        :\",decoded_translation)\n","  return decoded_translation"],"execution_count":18,"outputs":[]},{"cell_type":"code","metadata":{"id":"CLunc3y0Gvnr","executionInfo":{"status":"ok","timestamp":1601147399662,"user_tz":-330,"elapsed":1796,"user":{"displayName":"Sanket Sonje","photoUrl":"","userId":"05822068833245189120"}},"outputId":"ec73f154-c977-4381-c27c-21d63ad8aa20","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# testting for pickle\n","input = \"who\"\n","print(Predictions(input))"],"execution_count":22,"outputs":[{"output_type":"stream","text":[" bianca\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"1QG59P-tGvgI","executionInfo":{"status":"ok","timestamp":1601147409131,"user_tz":-330,"elapsed":898,"user":{"displayName":"Sanket Sonje","photoUrl":"","userId":"05822068833245189120"}}},"source":["import numpy as np\n","from flask import Flask, request, jsonify, render_template\n","import pickle\n","import threading"],"execution_count":23,"outputs":[]},{"cell_type":"code","metadata":{"id":"pdMuW0k5JAhE"},"source":["app = Flask(__name__)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NEI2DOj4GOKp"},"source":["@app.route('/', methods = ['POST'])\n","def response():\n","\n","\tuser_query = request.json.get('user_query')\n","\toutput_response = Prediction(user_query)\n"," \n","\treturn jsonify({'user_query' : output_response})"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"S0G2xzeyIz12"},"source":["if __name__ == '__main__':\n","\tapp.run()"],"execution_count":null,"outputs":[]}]}